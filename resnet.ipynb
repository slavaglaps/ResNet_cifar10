{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''ResNet50 model for Keras.\n",
    "# Reference:\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "Adapted from code contributed by BigMoyan.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow\n",
    "\n",
    "%env KERAS_BACKEND=tensorflow\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.engine.topology import get_source_inputs\n",
    "\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "  \n",
    "\n",
    "    x = Conv2D(filters1, (1, 1))(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same')(x)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        \n",
    "    x = Conv2D(filters1, (1, 1), strides=strides)(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization(axis=bn_axis)(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides)(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis)(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResNet50(input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (32,32,3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    \n",
    "    block_shape = K.int_shape(x)\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(block_shape[1], block_shape[2]),strides=(1, 1))(x)\n",
    "\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "def ResNet50_cifar(input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (32,32,3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Conv2D(16, (3, 3), strides=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = conv_block(x, 3, [16, 16, 64], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [16, 16, 64], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [16, 16, 64], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [32, 32, 128], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [32, 32, 128], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [32, 32, 128], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [32, 32, 128], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=5, block='c')\n",
    "    \n",
    "    block_shape = K.int_shape(x)\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(block_shape[1], block_shape[2]),strides=(1, 1))(x)\n",
    "\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "def ResNet50_cifar_small(input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (32,32,3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D((1, 1))(img_input)\n",
    "    x = Conv2D(16, (3, 3), strides=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    block_shape = K.int_shape(x)\n",
    "    print(block_shape)\n",
    "    x = conv_block(x, 3, [16, 16, 64], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [16, 16, 64], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [16, 16, 64], stage=2, block='c')\n",
    "    block_shape = K.int_shape(x)\n",
    "    print(block_shape)\n",
    "    x = conv_block(x, 3, [32, 32, 128], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [32, 32, 128], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [32, 32, 128], stage=3, block='c')\n",
    "    block_shape = K.int_shape(x)\n",
    "    print(block_shape)\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=4, block='c')\n",
    "    \n",
    "    \n",
    "    block_shape = K.int_shape(x)\n",
    "    print()\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(block_shape[1], block_shape[2]),strides=(1, 1))(x)\n",
    "\n",
    "   \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " model = ResNet50(classes = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 60s - loss: 2.4729 - acc: 0.2055 - val_loss: 2.2552 - val_acc: 0.1932\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 49s - loss: 1.8500 - acc: 0.3340 - val_loss: 2.1240 - val_acc: 0.3474\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 49s - loss: 1.5394 - acc: 0.4431 - val_loss: 1.5906 - val_acc: 0.4459\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 49s - loss: 1.3877 - acc: 0.4984 - val_loss: 1.3233 - val_acc: 0.5320\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 49s - loss: 1.2775 - acc: 0.5422 - val_loss: 1.3034 - val_acc: 0.5491\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 49s - loss: 1.1913 - acc: 0.5759 - val_loss: 1.6572 - val_acc: 0.4607\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 49s - loss: 1.1166 - acc: 0.6019 - val_loss: 1.1349 - val_acc: 0.6028\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 49s - loss: 1.0496 - acc: 0.6270 - val_loss: 1.0502 - val_acc: 0.6298\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 49s - loss: 1.0013 - acc: 0.6444 - val_loss: 1.0110 - val_acc: 0.6403\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 49s - loss: 0.9522 - acc: 0.6639 - val_loss: 1.3673 - val_acc: 0.5519\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 49s - loss: 0.9095 - acc: 0.6782 - val_loss: 1.0087 - val_acc: 0.6517\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 49s - loss: 0.8746 - acc: 0.6893 - val_loss: 0.9707 - val_acc: 0.6564\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 49s - loss: 0.8346 - acc: 0.7037 - val_loss: 0.9772 - val_acc: 0.6624\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 49s - loss: 0.8064 - acc: 0.7141 - val_loss: 1.0068 - val_acc: 0.6696\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 49s - loss: 0.7760 - acc: 0.7266 - val_loss: 1.0285 - val_acc: 0.6545\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 49s - loss: 0.7523 - acc: 0.7353 - val_loss: 0.9751 - val_acc: 0.6780\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 49s - loss: 0.7217 - acc: 0.7458 - val_loss: 0.8505 - val_acc: 0.7068\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 49s - loss: 0.7048 - acc: 0.7514 - val_loss: 0.9876 - val_acc: 0.6750\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 49s - loss: 0.6704 - acc: 0.7624 - val_loss: 0.9258 - val_acc: 0.6811\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 49s - loss: 0.6497 - acc: 0.7693 - val_loss: 1.2689 - val_acc: 0.6070\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 49s - loss: 0.6361 - acc: 0.7761 - val_loss: 1.0391 - val_acc: 0.6679\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 49s - loss: 0.6135 - acc: 0.7818 - val_loss: 0.8643 - val_acc: 0.7168\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 49s - loss: 0.5976 - acc: 0.7867 - val_loss: 0.7908 - val_acc: 0.7332\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 49s - loss: 0.5710 - acc: 0.7966 - val_loss: 1.0244 - val_acc: 0.6683\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 49s - loss: 0.5581 - acc: 0.8018 - val_loss: 0.7849 - val_acc: 0.7400\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 49s - loss: 0.5397 - acc: 0.8061 - val_loss: 0.7835 - val_acc: 0.7407\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 49s - loss: 0.5181 - acc: 0.8152 - val_loss: 0.9763 - val_acc: 0.6971\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 49s - loss: 0.5124 - acc: 0.8180 - val_loss: 0.9878 - val_acc: 0.6944\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 49s - loss: 0.4925 - acc: 0.8271 - val_loss: 0.8870 - val_acc: 0.7202\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 49s - loss: 0.4798 - acc: 0.8305 - val_loss: 0.8282 - val_acc: 0.7317\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 49s - loss: 0.4720 - acc: 0.8327 - val_loss: 0.8700 - val_acc: 0.7291\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 49s - loss: 0.4627 - acc: 0.8369 - val_loss: 0.8153 - val_acc: 0.7388\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 49s - loss: 0.4437 - acc: 0.8420 - val_loss: 0.8630 - val_acc: 0.7195\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 49s - loss: 0.4362 - acc: 0.8456 - val_loss: 0.7979 - val_acc: 0.7520\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 49s - loss: 0.4205 - acc: 0.8491 - val_loss: 0.7619 - val_acc: 0.7650\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 49s - loss: 0.4065 - acc: 0.8569 - val_loss: 0.8158 - val_acc: 0.7534\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 49s - loss: 0.3995 - acc: 0.8587 - val_loss: 0.8531 - val_acc: 0.7363\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 49s - loss: 0.3859 - acc: 0.8612 - val_loss: 0.8741 - val_acc: 0.7335\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 49s - loss: 0.3790 - acc: 0.8648 - val_loss: 0.8070 - val_acc: 0.7554\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 49s - loss: 0.3663 - acc: 0.8702 - val_loss: 0.8403 - val_acc: 0.7519\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 49s - loss: 0.3620 - acc: 0.8710 - val_loss: 0.7966 - val_acc: 0.7664\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 49s - loss: 0.3427 - acc: 0.8788 - val_loss: 0.7792 - val_acc: 0.7689\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 49s - loss: 0.3381 - acc: 0.8795 - val_loss: 0.7978 - val_acc: 0.7654\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 49s - loss: 0.3366 - acc: 0.8792 - val_loss: 0.7987 - val_acc: 0.7719\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 49s - loss: 0.3212 - acc: 0.8856 - val_loss: 0.8970 - val_acc: 0.7554\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 49s - loss: 0.3136 - acc: 0.8886 - val_loss: 0.8421 - val_acc: 0.7688\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 49s - loss: 0.3047 - acc: 0.8922 - val_loss: 0.7918 - val_acc: 0.7860\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 49s - loss: 0.2996 - acc: 0.8942 - val_loss: 0.8353 - val_acc: 0.7719\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 49s - loss: 0.2914 - acc: 0.8959 - val_loss: 0.8975 - val_acc: 0.7602\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 49s - loss: 0.2872 - acc: 0.8977 - val_loss: 0.9429 - val_acc: 0.7497\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 49s - loss: 0.2827 - acc: 0.8998 - val_loss: 0.8359 - val_acc: 0.7772\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 49s - loss: 0.2713 - acc: 0.9060 - val_loss: 0.7774 - val_acc: 0.7787\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 49s - loss: 0.2652 - acc: 0.9043 - val_loss: 0.9699 - val_acc: 0.7536\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 49s - loss: 0.2652 - acc: 0.9052 - val_loss: 0.8874 - val_acc: 0.7661\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 49s - loss: 0.2534 - acc: 0.9103 - val_loss: 0.9560 - val_acc: 0.7726\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 49s - loss: 0.2509 - acc: 0.9123 - val_loss: 0.8559 - val_acc: 0.7710\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 49s - loss: 0.2455 - acc: 0.9136 - val_loss: 0.9788 - val_acc: 0.7577\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 49s - loss: 0.2412 - acc: 0.9157 - val_loss: 0.9392 - val_acc: 0.7722\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 49s - loss: 0.2351 - acc: 0.9174 - val_loss: 0.8726 - val_acc: 0.7724\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 49s - loss: 0.2261 - acc: 0.9214 - val_loss: 1.1763 - val_acc: 0.7337\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 49s - loss: 0.2247 - acc: 0.9210 - val_loss: 0.8304 - val_acc: 0.7887\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 49s - loss: 0.2212 - acc: 0.9222 - val_loss: 1.0186 - val_acc: 0.7643\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 49s - loss: 0.2192 - acc: 0.9230 - val_loss: 0.9181 - val_acc: 0.7791\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 49s - loss: 0.2161 - acc: 0.9252 - val_loss: 0.8902 - val_acc: 0.7880\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 49s - loss: 0.2077 - acc: 0.9276 - val_loss: 1.1130 - val_acc: 0.7579\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 49s - loss: 0.2021 - acc: 0.9289 - val_loss: 0.9889 - val_acc: 0.7786\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 49s - loss: 0.2025 - acc: 0.9295 - val_loss: 0.8573 - val_acc: 0.7782\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 49s - loss: 0.1933 - acc: 0.9307 - val_loss: 0.9279 - val_acc: 0.7858\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 49s - loss: 0.1931 - acc: 0.9337 - val_loss: 1.1510 - val_acc: 0.7429\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 49s - loss: 0.1925 - acc: 0.9328 - val_loss: 0.9661 - val_acc: 0.7652\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 49s - loss: 0.1825 - acc: 0.9374 - val_loss: 1.0358 - val_acc: 0.7599\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 49s - loss: 0.1830 - acc: 0.9369 - val_loss: 0.9303 - val_acc: 0.7828\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 49s - loss: 0.1796 - acc: 0.9377 - val_loss: 0.9865 - val_acc: 0.7766\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 49s - loss: 0.1798 - acc: 0.9375 - val_loss: 0.9209 - val_acc: 0.7876\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 49s - loss: 0.1777 - acc: 0.9376 - val_loss: 0.9295 - val_acc: 0.7768\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 49s - loss: 0.1731 - acc: 0.9402 - val_loss: 1.1212 - val_acc: 0.7624\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 49s - loss: 0.1737 - acc: 0.9402 - val_loss: 1.0501 - val_acc: 0.7674\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 49s - loss: 0.1686 - acc: 0.9417 - val_loss: 1.1933 - val_acc: 0.7385\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 49s - loss: 0.1669 - acc: 0.9412 - val_loss: 0.9103 - val_acc: 0.7855\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 49s - loss: 0.1659 - acc: 0.9434 - val_loss: 0.9963 - val_acc: 0.7690\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 49s - loss: 0.1559 - acc: 0.9465 - val_loss: 0.9765 - val_acc: 0.7819\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 49s - loss: 0.1595 - acc: 0.9450 - val_loss: 1.1622 - val_acc: 0.7567\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 49s - loss: 0.1556 - acc: 0.9460 - val_loss: 1.1750 - val_acc: 0.7387\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 49s - loss: 0.1535 - acc: 0.9458 - val_loss: 0.9967 - val_acc: 0.7725\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 49s - loss: 0.1534 - acc: 0.9465 - val_loss: 0.9501 - val_acc: 0.7809\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 49s - loss: 0.1514 - acc: 0.9477 - val_loss: 1.2033 - val_acc: 0.7561\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 49s - loss: 0.1471 - acc: 0.9494 - val_loss: 1.0352 - val_acc: 0.7740\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 49s - loss: 0.1451 - acc: 0.9491 - val_loss: 0.9626 - val_acc: 0.7890\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 49s - loss: 0.1493 - acc: 0.9491 - val_loss: 0.9376 - val_acc: 0.7839\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 49s - loss: 0.1463 - acc: 0.9485 - val_loss: 1.0319 - val_acc: 0.7818\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 49s - loss: 0.1393 - acc: 0.9511 - val_loss: 1.1059 - val_acc: 0.7629\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 49s - loss: 0.1355 - acc: 0.9524 - val_loss: 1.0945 - val_acc: 0.7715\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 49s - loss: 0.1366 - acc: 0.9537 - val_loss: 1.1181 - val_acc: 0.7647\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 49s - loss: 0.1338 - acc: 0.9531 - val_loss: 0.9922 - val_acc: 0.7867\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 49s - loss: 0.1316 - acc: 0.9539 - val_loss: 1.0086 - val_acc: 0.7752\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 49s - loss: 0.1305 - acc: 0.9547 - val_loss: 1.1091 - val_acc: 0.7654\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 49s - loss: 0.1335 - acc: 0.9531 - val_loss: 1.0258 - val_acc: 0.7806\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 49s - loss: 0.1287 - acc: 0.9554 - val_loss: 0.9745 - val_acc: 0.7873\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 49s - loss: 0.1281 - acc: 0.9561 - val_loss: 1.1065 - val_acc: 0.7773\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 49s - loss: 0.1288 - acc: 0.9557 - val_loss: 1.2008 - val_acc: 0.7590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa7e679710>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=128),\n",
    "                        steps_per_epoch=x_train.shape[0] // 128,\n",
    "                        epochs=100,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 67s - loss: 2.4457 - acc: 0.1961 - val_loss: 2.7546 - val_acc: 0.1068\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 54s - loss: 1.9445 - acc: 0.2921 - val_loss: 1.8007 - val_acc: 0.3531\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 53s - loss: 1.7740 - acc: 0.3539 - val_loss: 1.7901 - val_acc: 0.3819\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 53s - loss: 1.6720 - acc: 0.3930 - val_loss: 1.6148 - val_acc: 0.4188\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 53s - loss: 1.5971 - acc: 0.4197 - val_loss: 1.5636 - val_acc: 0.4463\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 54s - loss: 1.5334 - acc: 0.4414 - val_loss: 1.4703 - val_acc: 0.4757\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 54s - loss: 1.4686 - acc: 0.4663 - val_loss: 1.4617 - val_acc: 0.4822\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 54s - loss: 1.4139 - acc: 0.4910 - val_loss: 1.4032 - val_acc: 0.4982\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 54s - loss: 1.3792 - acc: 0.5032 - val_loss: 1.3477 - val_acc: 0.5231\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 54s - loss: 1.3290 - acc: 0.5214 - val_loss: 1.2917 - val_acc: 0.5386\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 54s - loss: 1.2778 - acc: 0.5398 - val_loss: 1.2392 - val_acc: 0.5534\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 54s - loss: 1.2383 - acc: 0.5550 - val_loss: 1.2702 - val_acc: 0.5418\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 54s - loss: 1.1880 - acc: 0.5769 - val_loss: 1.2876 - val_acc: 0.5458\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 54s - loss: 1.1480 - acc: 0.5902 - val_loss: 1.1999 - val_acc: 0.5759\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 54s - loss: 1.1145 - acc: 0.6013 - val_loss: 1.1262 - val_acc: 0.5980\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 54s - loss: 1.0683 - acc: 0.6167 - val_loss: 1.2894 - val_acc: 0.5561\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 54s - loss: 1.0242 - acc: 0.6347 - val_loss: 1.0835 - val_acc: 0.6181\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 54s - loss: 0.9920 - acc: 0.6469 - val_loss: 1.1810 - val_acc: 0.6019\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 54s - loss: 0.9511 - acc: 0.6613 - val_loss: 1.0601 - val_acc: 0.6349\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 54s - loss: 0.9181 - acc: 0.6749 - val_loss: 1.1303 - val_acc: 0.6089\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 54s - loss: 0.8810 - acc: 0.6841 - val_loss: 0.9997 - val_acc: 0.6481\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 54s - loss: 0.8529 - acc: 0.6968 - val_loss: 0.9353 - val_acc: 0.6732\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 54s - loss: 0.8134 - acc: 0.7112 - val_loss: 0.9432 - val_acc: 0.6783\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 54s - loss: 0.7979 - acc: 0.7164 - val_loss: 0.9699 - val_acc: 0.6618\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 54s - loss: 0.7626 - acc: 0.7299 - val_loss: 0.9485 - val_acc: 0.6743\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 54s - loss: 0.7378 - acc: 0.7394 - val_loss: 0.8846 - val_acc: 0.6971\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 54s - loss: 0.7006 - acc: 0.7494 - val_loss: 0.9673 - val_acc: 0.6737\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 54s - loss: 0.6865 - acc: 0.7570 - val_loss: 0.8775 - val_acc: 0.7060\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 54s - loss: 0.6671 - acc: 0.7632 - val_loss: 0.9079 - val_acc: 0.7073\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 54s - loss: 0.6403 - acc: 0.7741 - val_loss: 0.9594 - val_acc: 0.7006\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 54s - loss: 0.6180 - acc: 0.7807 - val_loss: 0.8516 - val_acc: 0.7087\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 54s - loss: 0.6028 - acc: 0.7874 - val_loss: 0.8298 - val_acc: 0.7209\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 54s - loss: 0.5722 - acc: 0.7966 - val_loss: 1.0200 - val_acc: 0.6676\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 54s - loss: 0.5569 - acc: 0.8036 - val_loss: 0.8037 - val_acc: 0.7363\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 54s - loss: 0.5521 - acc: 0.8044 - val_loss: 0.8171 - val_acc: 0.7333\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 54s - loss: 0.5272 - acc: 0.8133 - val_loss: 0.7848 - val_acc: 0.7418\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 54s - loss: 0.5079 - acc: 0.8207 - val_loss: 0.9211 - val_acc: 0.7139\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 54s - loss: 0.4875 - acc: 0.8271 - val_loss: 0.8051 - val_acc: 0.7372\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 54s - loss: 0.4767 - acc: 0.8322 - val_loss: 0.8696 - val_acc: 0.7227\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 54s - loss: 0.4600 - acc: 0.8362 - val_loss: 0.7969 - val_acc: 0.7435\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 54s - loss: 0.4450 - acc: 0.8424 - val_loss: 0.8203 - val_acc: 0.7356\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 54s - loss: 0.4319 - acc: 0.8460 - val_loss: 0.7725 - val_acc: 0.7545\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 54s - loss: 0.4211 - acc: 0.8511 - val_loss: 0.9266 - val_acc: 0.7179\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 54s - loss: 0.4145 - acc: 0.8540 - val_loss: 0.9195 - val_acc: 0.7273\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 54s - loss: 0.4136 - acc: 0.8539 - val_loss: 0.8228 - val_acc: 0.7409\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 54s - loss: 0.3807 - acc: 0.8653 - val_loss: 0.8074 - val_acc: 0.7516\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 54s - loss: 0.3639 - acc: 0.8707 - val_loss: 0.8733 - val_acc: 0.7426\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 54s - loss: 0.3588 - acc: 0.8730 - val_loss: 0.8194 - val_acc: 0.7495\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 54s - loss: 0.3397 - acc: 0.8795 - val_loss: 0.8024 - val_acc: 0.7561\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 54s - loss: 0.3336 - acc: 0.8820 - val_loss: 1.5400 - val_acc: 0.6127\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 54s - loss: 0.3630 - acc: 0.8722 - val_loss: 0.7876 - val_acc: 0.7573\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 54s - loss: 0.3188 - acc: 0.8857 - val_loss: 0.8794 - val_acc: 0.7511\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 54s - loss: 0.3032 - acc: 0.8923 - val_loss: 0.9026 - val_acc: 0.7430\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 54s - loss: 0.4265 - acc: 0.8532 - val_loss: 0.7864 - val_acc: 0.7547\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 54s - loss: 0.3972 - acc: 0.8641 - val_loss: 1.0259 - val_acc: 0.6846\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 54s - loss: 0.4527 - acc: 0.8481 - val_loss: 0.7785 - val_acc: 0.7660\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 54s - loss: 0.3198 - acc: 0.8865 - val_loss: 0.7962 - val_acc: 0.7704\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 54s - loss: 0.2861 - acc: 0.8980 - val_loss: 0.8867 - val_acc: 0.7459\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 54s - loss: 0.2754 - acc: 0.9032 - val_loss: 0.8899 - val_acc: 0.7558\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 54s - loss: 0.2606 - acc: 0.9073 - val_loss: 0.9297 - val_acc: 0.7498\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 54s - loss: 0.3306 - acc: 0.8844 - val_loss: 0.8396 - val_acc: 0.7428\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 54s - loss: 0.2809 - acc: 0.8996 - val_loss: 0.8050 - val_acc: 0.7743\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 54s - loss: 0.2479 - acc: 0.9109 - val_loss: 0.9030 - val_acc: 0.7595\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 54s - loss: 0.2326 - acc: 0.9171 - val_loss: 0.9628 - val_acc: 0.7508\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 54s - loss: 0.2459 - acc: 0.9117 - val_loss: 0.8668 - val_acc: 0.7637\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 53s - loss: 0.2559 - acc: 0.9097 - val_loss: 0.9228 - val_acc: 0.7555\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 53s - loss: 0.3466 - acc: 0.8830 - val_loss: 0.8335 - val_acc: 0.7645\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 53s - loss: 0.2520 - acc: 0.9104 - val_loss: 0.9002 - val_acc: 0.7574\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 53s - loss: 0.2251 - acc: 0.9196 - val_loss: 0.9178 - val_acc: 0.7522\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 53s - loss: 0.2082 - acc: 0.9258 - val_loss: 0.9025 - val_acc: 0.7628\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 54s - loss: 0.2001 - acc: 0.9292 - val_loss: 0.9685 - val_acc: 0.7516\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 53s - loss: 0.2045 - acc: 0.9284 - val_loss: 0.9560 - val_acc: 0.7544\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 53s - loss: 0.1936 - acc: 0.9313 - val_loss: 1.0225 - val_acc: 0.7398\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 53s - loss: 0.1877 - acc: 0.9333 - val_loss: 0.9185 - val_acc: 0.7664\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 54s - loss: 0.1904 - acc: 0.9324 - val_loss: 1.0407 - val_acc: 0.7465\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 53s - loss: 0.1869 - acc: 0.9335 - val_loss: 1.0738 - val_acc: 0.7390\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 53s - loss: 0.1796 - acc: 0.9363 - val_loss: 0.9448 - val_acc: 0.7601\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 54s - loss: 0.1831 - acc: 0.9348 - val_loss: 1.0065 - val_acc: 0.7472\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 54s - loss: 0.2035 - acc: 0.9282 - val_loss: 0.9913 - val_acc: 0.7504\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 53s - loss: 0.1983 - acc: 0.9315 - val_loss: 0.9296 - val_acc: 0.7668\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 54s - loss: 0.1707 - acc: 0.9403 - val_loss: 0.9910 - val_acc: 0.7535\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 54s - loss: 0.1655 - acc: 0.9418 - val_loss: 1.1061 - val_acc: 0.7532\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 54s - loss: 0.1632 - acc: 0.9432 - val_loss: 0.9471 - val_acc: 0.7734\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 54s - loss: 0.2462 - acc: 0.9143 - val_loss: 0.9554 - val_acc: 0.7505\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 54s - loss: 0.1707 - acc: 0.9404 - val_loss: 0.9218 - val_acc: 0.7626\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 54s - loss: 0.1528 - acc: 0.9466 - val_loss: 0.9380 - val_acc: 0.7770\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 54s - loss: 0.1459 - acc: 0.9479 - val_loss: 1.0032 - val_acc: 0.7650\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 53s - loss: 0.1472 - acc: 0.9495 - val_loss: 1.2157 - val_acc: 0.7246\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 54s - loss: 0.1881 - acc: 0.9344 - val_loss: 0.9239 - val_acc: 0.7778\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 54s - loss: 0.1415 - acc: 0.9502 - val_loss: 0.9480 - val_acc: 0.7697\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 54s - loss: 0.1387 - acc: 0.9514 - val_loss: 0.9183 - val_acc: 0.7743\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 54s - loss: 0.1368 - acc: 0.9528 - val_loss: 0.9826 - val_acc: 0.7721\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 54s - loss: 0.1375 - acc: 0.9523 - val_loss: 1.0413 - val_acc: 0.7647\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 53s - loss: 0.1299 - acc: 0.9537 - val_loss: 1.2201 - val_acc: 0.7225\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 54s - loss: 0.1335 - acc: 0.9533 - val_loss: 1.0434 - val_acc: 0.7570\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 54s - loss: 0.1307 - acc: 0.9546 - val_loss: 1.2322 - val_acc: 0.7269\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 54s - loss: 0.1299 - acc: 0.9549 - val_loss: 1.0022 - val_acc: 0.7706\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 54s - loss: 0.1299 - acc: 0.9545 - val_loss: 1.0541 - val_acc: 0.7694\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 53s - loss: 0.1230 - acc: 0.9565 - val_loss: 1.2471 - val_acc: 0.7469\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 54s - loss: 0.1310 - acc: 0.9548 - val_loss: 1.0577 - val_acc: 0.7609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab14405e48>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=128),\n",
    "                        steps_per_epoch=x_train.shape[0] // 128,\n",
    "                        epochs=100,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " model1 = ResNet50_cifar(classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 61s - loss: 2.2006 - acc: 0.2005 - val_loss: 3.2887 - val_acc: 0.1007\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 45s - loss: 1.9569 - acc: 0.2781 - val_loss: 1.8985 - val_acc: 0.3102\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 45s - loss: 1.8469 - acc: 0.3203 - val_loss: 1.8144 - val_acc: 0.3418\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 45s - loss: 1.7609 - acc: 0.3567 - val_loss: 1.7162 - val_acc: 0.3865\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 45s - loss: 1.6951 - acc: 0.3809 - val_loss: 1.6371 - val_acc: 0.4061\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 45s - loss: 1.6339 - acc: 0.4070 - val_loss: 1.5792 - val_acc: 0.4297\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 45s - loss: 1.5896 - acc: 0.4236 - val_loss: 1.5100 - val_acc: 0.4506\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 45s - loss: 1.5443 - acc: 0.4388 - val_loss: 1.4871 - val_acc: 0.4567\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 45s - loss: 1.5010 - acc: 0.4547 - val_loss: 1.4531 - val_acc: 0.4734\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 45s - loss: 1.4666 - acc: 0.4672 - val_loss: 1.4096 - val_acc: 0.4840\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 45s - loss: 1.4266 - acc: 0.4806 - val_loss: 1.3745 - val_acc: 0.5019\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 45s - loss: 1.3918 - acc: 0.4958 - val_loss: 1.3519 - val_acc: 0.5113\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 45s - loss: 1.3640 - acc: 0.5041 - val_loss: 1.3228 - val_acc: 0.5213\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 45s - loss: 1.3316 - acc: 0.5182 - val_loss: 1.2913 - val_acc: 0.5335\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 45s - loss: 1.3045 - acc: 0.5252 - val_loss: 1.3343 - val_acc: 0.5167\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 45s - loss: 1.2821 - acc: 0.5343 - val_loss: 1.2669 - val_acc: 0.5384\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 45s - loss: 1.2477 - acc: 0.5462 - val_loss: 1.2425 - val_acc: 0.5505\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 45s - loss: 1.2255 - acc: 0.5548 - val_loss: 1.2402 - val_acc: 0.5530\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 46s - loss: 1.1986 - acc: 0.5667 - val_loss: 1.2012 - val_acc: 0.5625\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 45s - loss: 1.1710 - acc: 0.5793 - val_loss: 1.1968 - val_acc: 0.5718\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 45s - loss: 1.1512 - acc: 0.5836 - val_loss: 1.1745 - val_acc: 0.5777\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 47s - loss: 1.1346 - acc: 0.5890 - val_loss: 1.1610 - val_acc: 0.5831\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 46s - loss: 1.1070 - acc: 0.5991 - val_loss: 1.1633 - val_acc: 0.5840\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 46s - loss: 1.0912 - acc: 0.6101 - val_loss: 1.1417 - val_acc: 0.5893\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 45s - loss: 1.0748 - acc: 0.6100 - val_loss: 1.1168 - val_acc: 0.5959\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 45s - loss: 1.0500 - acc: 0.6200 - val_loss: 1.1276 - val_acc: 0.6001\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 46s - loss: 1.0319 - acc: 0.6279 - val_loss: 1.1001 - val_acc: 0.6126\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 45s - loss: 1.0181 - acc: 0.6322 - val_loss: 1.0844 - val_acc: 0.6161\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 45s - loss: 0.9960 - acc: 0.6420 - val_loss: 1.0985 - val_acc: 0.6092\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 46s - loss: 0.9797 - acc: 0.6458 - val_loss: 1.0532 - val_acc: 0.6255\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 46s - loss: 0.9635 - acc: 0.6542 - val_loss: 1.0501 - val_acc: 0.6267\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 45s - loss: 0.9463 - acc: 0.6596 - val_loss: 1.0989 - val_acc: 0.6128\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 46s - loss: 0.9341 - acc: 0.6638 - val_loss: 1.0459 - val_acc: 0.6279\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 46s - loss: 0.9192 - acc: 0.6681 - val_loss: 1.0560 - val_acc: 0.6288\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 45s - loss: 0.9039 - acc: 0.6761 - val_loss: 1.0409 - val_acc: 0.6318\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 45s - loss: 0.8883 - acc: 0.6821 - val_loss: 1.0288 - val_acc: 0.6359\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 46s - loss: 0.8767 - acc: 0.6826 - val_loss: 1.0360 - val_acc: 0.6376\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 46s - loss: 0.8617 - acc: 0.6909 - val_loss: 1.0085 - val_acc: 0.6482\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 45s - loss: 0.8495 - acc: 0.6959 - val_loss: 1.0280 - val_acc: 0.6414\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 45s - loss: 0.8338 - acc: 0.6998 - val_loss: 1.0277 - val_acc: 0.6457\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 45s - loss: 0.8245 - acc: 0.7034 - val_loss: 1.0075 - val_acc: 0.6485\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 46s - loss: 0.8085 - acc: 0.7109 - val_loss: 0.9864 - val_acc: 0.6559\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 46s - loss: 0.8009 - acc: 0.7142 - val_loss: 1.0149 - val_acc: 0.6546\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 45s - loss: 0.7830 - acc: 0.7193 - val_loss: 1.0476 - val_acc: 0.6424\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 46s - loss: 0.7736 - acc: 0.7211 - val_loss: 0.9986 - val_acc: 0.6534\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 45s - loss: 0.7616 - acc: 0.7268 - val_loss: 1.0085 - val_acc: 0.6524\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 46s - loss: 0.7511 - acc: 0.7301 - val_loss: 1.0344 - val_acc: 0.6470\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 45s - loss: 0.7344 - acc: 0.7350 - val_loss: 0.9838 - val_acc: 0.6607\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 46s - loss: 0.7299 - acc: 0.7379 - val_loss: 1.0050 - val_acc: 0.6585\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 45s - loss: 0.7152 - acc: 0.7422 - val_loss: 1.0112 - val_acc: 0.6545\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 46s - loss: 0.7052 - acc: 0.7454 - val_loss: 0.9918 - val_acc: 0.6598\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 46s - loss: 0.6903 - acc: 0.7522 - val_loss: 1.0093 - val_acc: 0.6570\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 45s - loss: 0.6929 - acc: 0.7516 - val_loss: 1.0305 - val_acc: 0.6602\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 46s - loss: 0.6797 - acc: 0.7575 - val_loss: 0.9853 - val_acc: 0.6641\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 45s - loss: 0.6684 - acc: 0.7591 - val_loss: 1.0049 - val_acc: 0.6666\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 46s - loss: 0.6529 - acc: 0.7653 - val_loss: 0.9900 - val_acc: 0.6680\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 45s - loss: 0.6472 - acc: 0.7680 - val_loss: 1.0027 - val_acc: 0.6659\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 46s - loss: 0.6382 - acc: 0.7710 - val_loss: 1.0322 - val_acc: 0.6595\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 45s - loss: 0.6289 - acc: 0.7754 - val_loss: 1.0230 - val_acc: 0.6596\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 45s - loss: 0.6228 - acc: 0.7765 - val_loss: 0.9970 - val_acc: 0.6677\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 45s - loss: 0.6117 - acc: 0.7786 - val_loss: 1.0489 - val_acc: 0.6584\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 45s - loss: 0.6005 - acc: 0.7856 - val_loss: 1.0991 - val_acc: 0.6569\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 45s - loss: 0.5916 - acc: 0.7879 - val_loss: 1.0787 - val_acc: 0.6599\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 46s - loss: 0.5829 - acc: 0.7928 - val_loss: 1.0208 - val_acc: 0.6735\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 45s - loss: 0.5786 - acc: 0.7893 - val_loss: 1.0188 - val_acc: 0.6733\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 45s - loss: 0.5672 - acc: 0.7968 - val_loss: 1.0066 - val_acc: 0.6814\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 45s - loss: 0.5605 - acc: 0.7988 - val_loss: 1.0743 - val_acc: 0.6642\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 45s - loss: 0.5501 - acc: 0.8030 - val_loss: 1.1000 - val_acc: 0.6573\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 45s - loss: 0.5372 - acc: 0.8070 - val_loss: 1.0653 - val_acc: 0.6702\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 45s - loss: 0.5329 - acc: 0.8092 - val_loss: 1.0237 - val_acc: 0.6752\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 45s - loss: 0.5315 - acc: 0.8116 - val_loss: 1.0731 - val_acc: 0.6663\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 45s - loss: 0.5184 - acc: 0.8135 - val_loss: 1.0523 - val_acc: 0.6697\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 45s - loss: 0.5094 - acc: 0.8184 - val_loss: 1.0174 - val_acc: 0.6773\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 45s - loss: 0.5058 - acc: 0.8186 - val_loss: 1.0684 - val_acc: 0.6763\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 45s - loss: 0.4929 - acc: 0.8240 - val_loss: 1.0535 - val_acc: 0.6772\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 45s - loss: 0.4882 - acc: 0.8257 - val_loss: 1.0701 - val_acc: 0.6713\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 45s - loss: 0.4836 - acc: 0.8252 - val_loss: 1.0637 - val_acc: 0.6733\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 45s - loss: 0.4840 - acc: 0.8270 - val_loss: 1.0407 - val_acc: 0.6820\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 45s - loss: 0.4671 - acc: 0.8316 - val_loss: 1.0785 - val_acc: 0.6757\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 45s - loss: 0.4643 - acc: 0.8350 - val_loss: 1.0976 - val_acc: 0.6740\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 45s - loss: 0.4530 - acc: 0.8382 - val_loss: 1.1092 - val_acc: 0.6705\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 45s - loss: 0.4433 - acc: 0.8408 - val_loss: 1.0737 - val_acc: 0.6789\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 45s - loss: 0.4415 - acc: 0.8417 - val_loss: 1.0951 - val_acc: 0.6795\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 45s - loss: 0.4376 - acc: 0.8418 - val_loss: 1.0989 - val_acc: 0.6754\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 45s - loss: 0.4342 - acc: 0.8452 - val_loss: 1.0873 - val_acc: 0.6819\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 45s - loss: 0.4280 - acc: 0.8470 - val_loss: 1.0903 - val_acc: 0.6836\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 45s - loss: 0.4226 - acc: 0.8488 - val_loss: 1.1524 - val_acc: 0.6709\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 45s - loss: 0.4165 - acc: 0.8527 - val_loss: 1.1145 - val_acc: 0.6721\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 45s - loss: 0.4070 - acc: 0.8551 - val_loss: 1.1082 - val_acc: 0.6767\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 45s - loss: 0.4059 - acc: 0.8553 - val_loss: 1.1434 - val_acc: 0.6689\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 45s - loss: 0.4015 - acc: 0.8558 - val_loss: 1.1140 - val_acc: 0.6841\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 45s - loss: 0.3924 - acc: 0.8589 - val_loss: 1.1118 - val_acc: 0.6797\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 45s - loss: 0.3855 - acc: 0.8632 - val_loss: 1.1205 - val_acc: 0.6815\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 45s - loss: 0.3861 - acc: 0.8631 - val_loss: 1.1516 - val_acc: 0.6700\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 45s - loss: 0.3735 - acc: 0.8678 - val_loss: 1.1538 - val_acc: 0.6763\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 45s - loss: 0.3729 - acc: 0.8668 - val_loss: 1.1588 - val_acc: 0.6740\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 45s - loss: 0.3693 - acc: 0.8676 - val_loss: 1.1478 - val_acc: 0.6781\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 45s - loss: 0.3678 - acc: 0.8679 - val_loss: 1.1761 - val_acc: 0.6728\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 45s - loss: 0.3542 - acc: 0.8738 - val_loss: 1.1647 - val_acc: 0.6817\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 45s - loss: 0.3561 - acc: 0.8741 - val_loss: 1.1663 - val_acc: 0.6749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ac10c2aac8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model1.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=128),\n",
    "                        steps_per_epoch=x_train.shape[0] // 128,\n",
    "                        epochs=100,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16, 16)\n",
      "(None, 16, 16, 64)\n",
      "(None, 8, 8, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " model2 = ResNet50_cifar_small(classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 46s - loss: 2.0845 - acc: 0.2424 - val_loss: 2.5170 - val_acc: 0.1538\n",
      "Epoch 2/100\n",
      "390/390 [==============================] - 38s - loss: 1.7754 - acc: 0.3465 - val_loss: 1.6713 - val_acc: 0.3895\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 38s - loss: 1.6571 - acc: 0.3914 - val_loss: 1.5829 - val_acc: 0.4223\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 38s - loss: 1.5869 - acc: 0.4173 - val_loss: 1.5334 - val_acc: 0.4355\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 38s - loss: 1.5285 - acc: 0.4401 - val_loss: 1.4574 - val_acc: 0.4640\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 38s - loss: 1.4804 - acc: 0.4571 - val_loss: 1.4258 - val_acc: 0.4759\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 38s - loss: 1.4397 - acc: 0.4733 - val_loss: 1.3816 - val_acc: 0.4932\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 38s - loss: 1.4026 - acc: 0.4878 - val_loss: 1.3560 - val_acc: 0.5055\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 38s - loss: 1.3677 - acc: 0.5025 - val_loss: 1.3072 - val_acc: 0.5242\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 38s - loss: 1.3369 - acc: 0.5162 - val_loss: 1.2784 - val_acc: 0.5326\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 38s - loss: 1.3089 - acc: 0.5257 - val_loss: 1.2915 - val_acc: 0.5278\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 38s - loss: 1.2760 - acc: 0.5368 - val_loss: 1.2311 - val_acc: 0.5522\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 38s - loss: 1.2576 - acc: 0.5446 - val_loss: 1.2219 - val_acc: 0.5594\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 38s - loss: 1.2312 - acc: 0.5557 - val_loss: 1.1979 - val_acc: 0.5713\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 38s - loss: 1.2095 - acc: 0.5639 - val_loss: 1.1729 - val_acc: 0.5783\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 38s - loss: 1.1840 - acc: 0.5733 - val_loss: 1.1536 - val_acc: 0.5836\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 38s - loss: 1.1659 - acc: 0.5826 - val_loss: 1.1601 - val_acc: 0.5809\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 38s - loss: 1.1525 - acc: 0.5866 - val_loss: 1.1324 - val_acc: 0.5928\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 38s - loss: 1.1319 - acc: 0.5932 - val_loss: 1.1107 - val_acc: 0.5994\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 38s - loss: 1.1120 - acc: 0.6013 - val_loss: 1.1035 - val_acc: 0.6020\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 38s - loss: 1.1002 - acc: 0.6065 - val_loss: 1.0814 - val_acc: 0.6140\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 38s - loss: 1.0817 - acc: 0.6137 - val_loss: 1.0825 - val_acc: 0.6160\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 38s - loss: 1.0705 - acc: 0.6166 - val_loss: 1.0823 - val_acc: 0.6104\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 38s - loss: 1.0582 - acc: 0.6237 - val_loss: 1.0609 - val_acc: 0.6224\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 38s - loss: 1.0409 - acc: 0.6265 - val_loss: 1.0483 - val_acc: 0.6282\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 38s - loss: 1.0323 - acc: 0.6310 - val_loss: 1.0447 - val_acc: 0.6279\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 38s - loss: 1.0151 - acc: 0.6365 - val_loss: 1.0740 - val_acc: 0.6179\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 38s - loss: 0.9994 - acc: 0.6411 - val_loss: 1.0116 - val_acc: 0.6423\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 38s - loss: 0.9904 - acc: 0.6446 - val_loss: 1.0164 - val_acc: 0.6389\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 38s - loss: 0.9799 - acc: 0.6504 - val_loss: 1.0012 - val_acc: 0.6456\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 38s - loss: 0.9676 - acc: 0.6539 - val_loss: 1.0193 - val_acc: 0.6425\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 38s - loss: 0.9633 - acc: 0.6573 - val_loss: 0.9826 - val_acc: 0.6523\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 38s - loss: 0.9358 - acc: 0.6691 - val_loss: 0.9667 - val_acc: 0.6574\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 38s - loss: 0.9354 - acc: 0.6669 - val_loss: 0.9635 - val_acc: 0.6560\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 38s - loss: 0.9267 - acc: 0.6705 - val_loss: 0.9897 - val_acc: 0.6496\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 38s - loss: 0.9130 - acc: 0.6755 - val_loss: 0.9528 - val_acc: 0.6612\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 38s - loss: 0.9044 - acc: 0.6774 - val_loss: 0.9424 - val_acc: 0.6659\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 38s - loss: 0.9020 - acc: 0.6784 - val_loss: 0.9364 - val_acc: 0.6634\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 38s - loss: 0.8806 - acc: 0.6855 - val_loss: 0.9467 - val_acc: 0.6654\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 38s - loss: 0.8799 - acc: 0.6878 - val_loss: 0.9273 - val_acc: 0.6721\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 38s - loss: 0.8643 - acc: 0.6924 - val_loss: 0.9161 - val_acc: 0.6765\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 38s - loss: 0.8577 - acc: 0.6956 - val_loss: 0.9085 - val_acc: 0.6759\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 38s - loss: 0.8517 - acc: 0.6956 - val_loss: 0.9134 - val_acc: 0.6784\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 38s - loss: 0.8411 - acc: 0.7022 - val_loss: 0.8892 - val_acc: 0.6852\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 38s - loss: 0.8312 - acc: 0.7040 - val_loss: 0.8975 - val_acc: 0.6839\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 38s - loss: 0.8269 - acc: 0.7060 - val_loss: 0.8949 - val_acc: 0.6817\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 38s - loss: 0.8222 - acc: 0.7065 - val_loss: 0.8923 - val_acc: 0.6858\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 38s - loss: 0.8074 - acc: 0.7133 - val_loss: 0.8687 - val_acc: 0.6945\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 38s - loss: 0.8050 - acc: 0.7139 - val_loss: 0.8843 - val_acc: 0.6917\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 38s - loss: 0.7917 - acc: 0.7190 - val_loss: 0.8906 - val_acc: 0.6891\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 38s - loss: 0.7899 - acc: 0.7195 - val_loss: 0.8671 - val_acc: 0.6953\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 38s - loss: 0.7773 - acc: 0.7241 - val_loss: 0.8631 - val_acc: 0.6958\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 38s - loss: 0.7669 - acc: 0.7277 - val_loss: 0.8729 - val_acc: 0.6995\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 38s - loss: 0.7694 - acc: 0.7255 - val_loss: 0.8526 - val_acc: 0.7050\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 38s - loss: 0.7547 - acc: 0.7321 - val_loss: 0.8427 - val_acc: 0.7077\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 38s - loss: 0.7528 - acc: 0.7331 - val_loss: 0.8506 - val_acc: 0.7055\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 38s - loss: 0.7431 - acc: 0.7350 - val_loss: 0.8382 - val_acc: 0.7072\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 38s - loss: 0.7412 - acc: 0.7381 - val_loss: 0.8435 - val_acc: 0.7069\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 38s - loss: 0.7342 - acc: 0.7391 - val_loss: 0.8460 - val_acc: 0.7043\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 38s - loss: 0.7289 - acc: 0.7404 - val_loss: 0.8189 - val_acc: 0.7121\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 37s - loss: 0.7212 - acc: 0.7441 - val_loss: 0.8489 - val_acc: 0.7045\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 37s - loss: 0.7170 - acc: 0.7442 - val_loss: 0.8215 - val_acc: 0.7105\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 37s - loss: 0.7145 - acc: 0.7447 - val_loss: 0.8414 - val_acc: 0.7054\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 37s - loss: 0.7052 - acc: 0.7509 - val_loss: 0.8315 - val_acc: 0.7072\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 38s - loss: 0.6969 - acc: 0.7527 - val_loss: 0.8223 - val_acc: 0.7125\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 37s - loss: 0.6944 - acc: 0.7512 - val_loss: 0.8179 - val_acc: 0.7175\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 37s - loss: 0.6848 - acc: 0.7559 - val_loss: 0.8091 - val_acc: 0.7185\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 38s - loss: 0.6880 - acc: 0.7573 - val_loss: 0.7986 - val_acc: 0.7234\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 38s - loss: 0.6772 - acc: 0.7586 - val_loss: 0.8018 - val_acc: 0.7225\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 38s - loss: 0.6675 - acc: 0.7642 - val_loss: 0.7934 - val_acc: 0.7264\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 38s - loss: 0.6651 - acc: 0.7650 - val_loss: 0.8207 - val_acc: 0.7176\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 38s - loss: 0.6623 - acc: 0.7631 - val_loss: 0.8030 - val_acc: 0.7206\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 38s - loss: 0.6589 - acc: 0.7660 - val_loss: 0.7935 - val_acc: 0.7232\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 38s - loss: 0.6516 - acc: 0.7674 - val_loss: 0.7929 - val_acc: 0.7245\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 39s - loss: 0.6463 - acc: 0.7694 - val_loss: 0.8032 - val_acc: 0.7226\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 38s - loss: 0.6426 - acc: 0.7713 - val_loss: 0.7881 - val_acc: 0.7274\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 38s - loss: 0.6354 - acc: 0.7754 - val_loss: 0.7926 - val_acc: 0.7275\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 38s - loss: 0.6339 - acc: 0.7762 - val_loss: 0.7850 - val_acc: 0.7275\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 38s - loss: 0.6273 - acc: 0.7760 - val_loss: 0.7901 - val_acc: 0.7243\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 38s - loss: 0.6216 - acc: 0.7781 - val_loss: 0.7762 - val_acc: 0.7332\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 38s - loss: 0.6159 - acc: 0.7819 - val_loss: 0.7794 - val_acc: 0.7323\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 38s - loss: 0.6136 - acc: 0.7812 - val_loss: 0.7977 - val_acc: 0.7278\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 38s - loss: 0.6096 - acc: 0.7852 - val_loss: 0.7582 - val_acc: 0.7365\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 38s - loss: 0.6063 - acc: 0.7830 - val_loss: 0.7863 - val_acc: 0.7257\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 38s - loss: 0.6037 - acc: 0.7853 - val_loss: 0.7998 - val_acc: 0.7226\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 38s - loss: 0.5996 - acc: 0.7856 - val_loss: 0.7663 - val_acc: 0.7344\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 38s - loss: 0.6012 - acc: 0.7852 - val_loss: 0.7554 - val_acc: 0.7400\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 38s - loss: 0.5875 - acc: 0.7918 - val_loss: 0.7671 - val_acc: 0.7377\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 38s - loss: 0.5918 - acc: 0.7892 - val_loss: 0.7510 - val_acc: 0.7443\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 38s - loss: 0.5838 - acc: 0.7930 - val_loss: 0.7590 - val_acc: 0.7447\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 38s - loss: 0.5792 - acc: 0.7939 - val_loss: 0.7548 - val_acc: 0.7385\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 38s - loss: 0.5793 - acc: 0.7943 - val_loss: 0.7838 - val_acc: 0.7326\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 38s - loss: 0.5623 - acc: 0.7992 - val_loss: 0.7633 - val_acc: 0.7403\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 38s - loss: 0.5648 - acc: 0.8010 - val_loss: 0.7658 - val_acc: 0.7362\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 38s - loss: 0.5652 - acc: 0.7976 - val_loss: 0.7489 - val_acc: 0.7430\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 38s - loss: 0.5634 - acc: 0.7987 - val_loss: 0.7360 - val_acc: 0.7500\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 38s - loss: 0.5549 - acc: 0.8042 - val_loss: 0.7542 - val_acc: 0.7392\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 38s - loss: 0.5468 - acc: 0.8048 - val_loss: 0.7353 - val_acc: 0.7453\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 38s - loss: 0.5493 - acc: 0.8030 - val_loss: 0.7662 - val_acc: 0.7408\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 38s - loss: 0.5434 - acc: 0.8077 - val_loss: 0.7368 - val_acc: 0.7480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abd7c857b8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model2.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=128),\n",
    "                        steps_per_epoch=x_train.shape[0] // 128,\n",
    "                        epochs=100,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16, 16)\n",
      "(None, 16, 16, 64)\n",
      "(None, 8, 8, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " model2 = ResNet50_cifar_small(classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "390/390 [==============================] - 54s - loss: 1.6663 - acc: 0.3808 - val_loss: 2.3174 - val_acc: 0.2637\n",
      "Epoch 2/70\n",
      "390/390 [==============================] - 39s - loss: 1.3447 - acc: 0.5119 - val_loss: 1.5011 - val_acc: 0.4614\n",
      "Epoch 3/70\n",
      "390/390 [==============================] - 39s - loss: 1.1906 - acc: 0.5705 - val_loss: 1.3068 - val_acc: 0.5478\n",
      "Epoch 4/70\n",
      "390/390 [==============================] - 39s - loss: 1.0834 - acc: 0.6123 - val_loss: 1.1053 - val_acc: 0.6077\n",
      "Epoch 5/70\n",
      "390/390 [==============================] - 39s - loss: 1.0007 - acc: 0.6418 - val_loss: 1.0135 - val_acc: 0.6369\n",
      "Epoch 6/70\n",
      "390/390 [==============================] - 39s - loss: 0.9238 - acc: 0.6699 - val_loss: 1.0549 - val_acc: 0.6301\n",
      "Epoch 7/70\n",
      "390/390 [==============================] - 39s - loss: 0.8693 - acc: 0.6923 - val_loss: 1.0320 - val_acc: 0.6481\n",
      "Epoch 8/70\n",
      "390/390 [==============================] - 39s - loss: 0.8160 - acc: 0.7112 - val_loss: 1.0217 - val_acc: 0.6535\n",
      "Epoch 9/70\n",
      "390/390 [==============================] - 39s - loss: 0.7723 - acc: 0.7263 - val_loss: 0.8210 - val_acc: 0.7089\n",
      "Epoch 10/70\n",
      "390/390 [==============================] - 39s - loss: 0.7311 - acc: 0.7412 - val_loss: 1.1312 - val_acc: 0.6290\n",
      "Epoch 11/70\n",
      "390/390 [==============================] - 39s - loss: 0.7007 - acc: 0.7537 - val_loss: 0.7582 - val_acc: 0.7316\n",
      "Epoch 12/70\n",
      "390/390 [==============================] - 39s - loss: 0.6675 - acc: 0.7647 - val_loss: 0.8374 - val_acc: 0.7118\n",
      "Epoch 13/70\n",
      "390/390 [==============================] - 39s - loss: 0.6464 - acc: 0.7717 - val_loss: 0.7388 - val_acc: 0.7441\n",
      "Epoch 14/70\n",
      "390/390 [==============================] - 39s - loss: 0.6221 - acc: 0.7816 - val_loss: 0.8593 - val_acc: 0.7080\n",
      "Epoch 15/70\n",
      "390/390 [==============================] - 39s - loss: 0.6021 - acc: 0.7878 - val_loss: 0.6695 - val_acc: 0.7676\n",
      "Epoch 16/70\n",
      "390/390 [==============================] - 39s - loss: 0.5835 - acc: 0.7942 - val_loss: 0.7650 - val_acc: 0.7455\n",
      "Epoch 17/70\n",
      "390/390 [==============================] - 39s - loss: 0.5644 - acc: 0.8014 - val_loss: 0.6687 - val_acc: 0.7672\n",
      "Epoch 18/70\n",
      "390/390 [==============================] - 39s - loss: 0.5556 - acc: 0.8038 - val_loss: 0.6890 - val_acc: 0.7638\n",
      "Epoch 19/70\n",
      "390/390 [==============================] - 39s - loss: 0.5376 - acc: 0.8087 - val_loss: 0.6830 - val_acc: 0.7718\n",
      "Epoch 20/70\n",
      "390/390 [==============================] - 39s - loss: 0.5193 - acc: 0.8181 - val_loss: 1.1311 - val_acc: 0.6670\n",
      "Epoch 21/70\n",
      "390/390 [==============================] - 39s - loss: 0.5066 - acc: 0.8237 - val_loss: 0.6870 - val_acc: 0.7755\n",
      "Epoch 22/70\n",
      "390/390 [==============================] - 39s - loss: 0.4999 - acc: 0.8237 - val_loss: 0.7672 - val_acc: 0.7481\n",
      "Epoch 23/70\n",
      "390/390 [==============================] - 39s - loss: 0.4885 - acc: 0.8279 - val_loss: 0.6251 - val_acc: 0.7892\n",
      "Epoch 24/70\n",
      "390/390 [==============================] - 39s - loss: 0.4787 - acc: 0.8305 - val_loss: 0.6036 - val_acc: 0.7930\n",
      "Epoch 25/70\n",
      "390/390 [==============================] - 39s - loss: 0.4652 - acc: 0.8365 - val_loss: 0.6202 - val_acc: 0.7902\n",
      "Epoch 26/70\n",
      "390/390 [==============================] - 39s - loss: 0.4580 - acc: 0.8384 - val_loss: 0.6610 - val_acc: 0.7848\n",
      "Epoch 27/70\n",
      "390/390 [==============================] - 39s - loss: 0.4493 - acc: 0.8419 - val_loss: 0.6232 - val_acc: 0.7919\n",
      "Epoch 28/70\n",
      "390/390 [==============================] - 39s - loss: 0.4358 - acc: 0.8456 - val_loss: 0.8253 - val_acc: 0.7337\n",
      "Epoch 29/70\n",
      "390/390 [==============================] - 39s - loss: 0.4330 - acc: 0.8466 - val_loss: 0.5674 - val_acc: 0.8080\n",
      "Epoch 30/70\n",
      "390/390 [==============================] - 39s - loss: 0.4234 - acc: 0.8501 - val_loss: 0.7436 - val_acc: 0.7596\n",
      "Epoch 31/70\n",
      "390/390 [==============================] - 39s - loss: 0.4110 - acc: 0.8556 - val_loss: 0.6312 - val_acc: 0.7904\n",
      "Epoch 32/70\n",
      "390/390 [==============================] - 39s - loss: 0.4093 - acc: 0.8555 - val_loss: 0.6902 - val_acc: 0.7810\n",
      "Epoch 33/70\n",
      "390/390 [==============================] - 39s - loss: 0.3979 - acc: 0.8576 - val_loss: 0.6651 - val_acc: 0.7882\n",
      "Epoch 34/70\n",
      "390/390 [==============================] - 39s - loss: 0.3999 - acc: 0.8583 - val_loss: 0.5882 - val_acc: 0.8107\n",
      "Epoch 35/70\n",
      "390/390 [==============================] - 39s - loss: 0.3882 - acc: 0.8639 - val_loss: 0.5371 - val_acc: 0.8187\n",
      "Epoch 36/70\n",
      "390/390 [==============================] - 39s - loss: 0.3777 - acc: 0.8659 - val_loss: 0.5637 - val_acc: 0.8165\n",
      "Epoch 37/70\n",
      "390/390 [==============================] - 39s - loss: 0.3709 - acc: 0.8690 - val_loss: 0.5427 - val_acc: 0.8200\n",
      "Epoch 38/70\n",
      "390/390 [==============================] - 39s - loss: 0.3718 - acc: 0.8678 - val_loss: 0.6364 - val_acc: 0.7977\n",
      "Epoch 39/70\n",
      "390/390 [==============================] - 39s - loss: 0.3604 - acc: 0.8718 - val_loss: 0.5698 - val_acc: 0.8123\n",
      "Epoch 40/70\n",
      "390/390 [==============================] - 39s - loss: 0.3573 - acc: 0.8729 - val_loss: 0.5852 - val_acc: 0.8094\n",
      "Epoch 41/70\n",
      "390/390 [==============================] - 39s - loss: 0.3477 - acc: 0.8751 - val_loss: 0.5361 - val_acc: 0.8280\n",
      "Epoch 42/70\n",
      "390/390 [==============================] - 39s - loss: 0.3472 - acc: 0.8761 - val_loss: 0.6638 - val_acc: 0.7866\n",
      "Epoch 43/70\n",
      "390/390 [==============================] - 39s - loss: 0.3385 - acc: 0.8797 - val_loss: 0.5462 - val_acc: 0.8230\n",
      "Epoch 44/70\n",
      "390/390 [==============================] - 39s - loss: 0.3367 - acc: 0.8807 - val_loss: 0.5100 - val_acc: 0.8340\n",
      "Epoch 45/70\n",
      "390/390 [==============================] - 39s - loss: 0.3319 - acc: 0.8814 - val_loss: 0.5734 - val_acc: 0.8171\n",
      "Epoch 46/70\n",
      "390/390 [==============================] - 39s - loss: 0.3224 - acc: 0.8859 - val_loss: 0.6135 - val_acc: 0.8014\n",
      "Epoch 47/70\n",
      "390/390 [==============================] - 39s - loss: 0.3199 - acc: 0.8865 - val_loss: 0.5527 - val_acc: 0.8239\n",
      "Epoch 48/70\n",
      "390/390 [==============================] - 39s - loss: 0.3176 - acc: 0.8872 - val_loss: 0.5061 - val_acc: 0.8367\n",
      "Epoch 49/70\n",
      "390/390 [==============================] - 39s - loss: 0.3177 - acc: 0.8881 - val_loss: 0.5718 - val_acc: 0.8133\n",
      "Epoch 50/70\n",
      "390/390 [==============================] - 39s - loss: 0.3113 - acc: 0.8884 - val_loss: 0.5236 - val_acc: 0.8343\n",
      "Epoch 51/70\n",
      "390/390 [==============================] - 39s - loss: 0.3024 - acc: 0.8916 - val_loss: 0.5355 - val_acc: 0.8281\n",
      "Epoch 52/70\n",
      "390/390 [==============================] - 39s - loss: 0.2978 - acc: 0.8945 - val_loss: 0.6046 - val_acc: 0.8102\n",
      "Epoch 53/70\n",
      "390/390 [==============================] - 39s - loss: 0.2974 - acc: 0.8937 - val_loss: 0.5696 - val_acc: 0.8216\n",
      "Epoch 54/70\n",
      "390/390 [==============================] - 39s - loss: 0.2964 - acc: 0.8953 - val_loss: 0.5727 - val_acc: 0.8202\n",
      "Epoch 55/70\n",
      "390/390 [==============================] - 39s - loss: 0.2867 - acc: 0.8981 - val_loss: 0.5378 - val_acc: 0.8342\n",
      "Epoch 56/70\n",
      "390/390 [==============================] - 39s - loss: 0.2846 - acc: 0.8976 - val_loss: 0.5526 - val_acc: 0.8276\n",
      "Epoch 57/70\n",
      "390/390 [==============================] - 39s - loss: 0.2860 - acc: 0.8977 - val_loss: 0.5664 - val_acc: 0.8281\n",
      "Epoch 58/70\n",
      "390/390 [==============================] - 39s - loss: 0.2783 - acc: 0.9003 - val_loss: 0.5234 - val_acc: 0.8350\n",
      "Epoch 59/70\n",
      "390/390 [==============================] - 39s - loss: 0.2754 - acc: 0.9016 - val_loss: 0.5572 - val_acc: 0.8344\n",
      "Epoch 60/70\n",
      "390/390 [==============================] - 39s - loss: 0.2683 - acc: 0.9040 - val_loss: 0.6238 - val_acc: 0.8063\n",
      "Epoch 61/70\n",
      "390/390 [==============================] - 39s - loss: 0.2730 - acc: 0.9017 - val_loss: 0.5353 - val_acc: 0.8310\n",
      "Epoch 62/70\n",
      "390/390 [==============================] - 39s - loss: 0.2650 - acc: 0.9054 - val_loss: 0.5313 - val_acc: 0.8396\n",
      "Epoch 63/70\n",
      "390/390 [==============================] - 39s - loss: 0.2611 - acc: 0.9060 - val_loss: 0.5410 - val_acc: 0.8361\n",
      "Epoch 64/70\n",
      "390/390 [==============================] - 39s - loss: 0.2600 - acc: 0.9075 - val_loss: 0.4956 - val_acc: 0.8458\n",
      "Epoch 65/70\n",
      "390/390 [==============================] - 39s - loss: 0.2542 - acc: 0.9087 - val_loss: 0.5950 - val_acc: 0.8211\n",
      "Epoch 66/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 39s - loss: 0.2529 - acc: 0.9099 - val_loss: 0.5266 - val_acc: 0.8359\n",
      "Epoch 67/70\n",
      "390/390 [==============================] - 39s - loss: 0.2464 - acc: 0.9109 - val_loss: 0.5899 - val_acc: 0.8255\n",
      "Epoch 68/70\n",
      "390/390 [==============================] - 39s - loss: 0.2492 - acc: 0.9092 - val_loss: 0.5301 - val_acc: 0.8367\n",
      "Epoch 69/70\n",
      "390/390 [==============================] - 39s - loss: 0.2458 - acc: 0.9120 - val_loss: 0.4951 - val_acc: 0.8483\n",
      "Epoch 70/70\n",
      "390/390 [==============================] - 39s - loss: 0.2436 - acc: 0.9139 - val_loss: 0.5853 - val_acc: 0.8294\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "histori = model2.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=128),\n",
    "                        steps_per_epoch=x_train.shape[0] // 128,\n",
    "                        epochs=70,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ac99e77240>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss', 'acc', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(histori.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0r6CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1acced32668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npVQjTQpKKaUaaVJQSinV6P8BBkiD7qBcR/AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ac99d47a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(histori.history['acc'])\n",
    "plt.plot(histori.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(histori.history['loss'])\n",
    "plt.plot(histori.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
